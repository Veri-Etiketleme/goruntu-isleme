{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Lane Lines on the Road \n",
    "\n",
    "When we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n",
    "\n",
    "In this project, we detect lane lines in videos using Python and OpenCV.  \n",
    "\n",
    "## Overall steps\n",
    "\n",
    "The method consists of the following steps:\n",
    "\n",
    "* Loading each image from the video clip\n",
    "* Grayscaling\n",
    "* Gaussian smoothing\n",
    "* Canny edge detection\n",
    "* Region masking\n",
    "* Applying Hough line transform\n",
    "* Overlaying lanes on original image\n",
    "\n",
    "## Lane detection methodology\n",
    "\n",
    "The method consists of the following steps:\n",
    "\n",
    "### 1) Loading each image from the video clip\n",
    "\n",
    "The method works on each image from the video clip. In the end, the processed images are framed together to generate a new \n",
    "video clip with the detected lanes superimposed on each frame of the video. It is always a good idea to see the type of input image and its respective dimensions. Plotting the image illustrates whether the image is square or rectangular. The original image is as follows:\n",
    "\n",
    "[image1]: ./figures/original_image.jpg \"Original image\"\n",
    "![alt text][image1]\n",
    "\n",
    "The image is of type numpy array with dimensions: (540, 960, 3).\n",
    "\n",
    "### 2) Grayscaling\n",
    "\n",
    "The input images are in RGB format. Many of the computer vision algorithms such as Canny edge detector operate on grayscaled images. The input image is converted into a grayscaled image using `cvtColor(img, COLOR_RGB2GRAY)` function from OpenCV. The grayscaled image is depicted as follows:\n",
    "\n",
    "[image2]: ./figures/grayscaled_image.jpg \"Grayscaled image\"\n",
    "![alt text][image2]\n",
    "\n",
    "*img* is the input RGB image.  \n",
    "\n",
    "### 3) Gaussian smoothing\n",
    "\n",
    "As a preprocessing step to Canny edge detection, the grayscaled image is smoothed using Gaussian blurring method using `GaussianBlur(img, (kernel_size, kernel_size), 0)` function from OpenCV. Though the Canny edge detection includes Gaussian blurring, doing it apriori allows flexibility to choose different filter kernel sizes. We used the *kernel size* of 5. The Gaussian smoothed image is depicted as follows:\n",
    "\n",
    "[image3]: ./figures/guassian_filtered_image.jpg \"Gaussian blurred image\"\n",
    "![alt text][image3]\n",
    "\n",
    "*img* is the grayscaled image.\n",
    "\n",
    "### 4) Canny edge detection\n",
    "\n",
    "Edges are detected on the Gaussian smoothed image using the Canny edge detector with the OpenCV function `Canny(img, low_threshold, high_threshold)`. Following edges were detected:\n",
    "\n",
    "[image4]: ./figures/canny_edge_image.jpg \"Canny edges\"\n",
    "![alt text][image4]\n",
    "\n",
    "*img* is the Gaussian smoothed *image*, *low threshold = 50*, and *high threshold = 150*.\n",
    "\n",
    "### 5) Region masking\n",
    "\n",
    "As seen in the above step, edges are detected all over the image, wherever there is a significant gradient change. To restrict the edges to specific region of an image where the lanes are most likely present, region mask is found using the function `fillPoly(mask, vertices, ignore_mask_color)` from OpenCV. The output of this step is depicted as follows:\n",
    "\n",
    "[image5]: ./figures/region_mask.jpg \"Region mask\"\n",
    "![alt text][image5]\n",
    "\n",
    "*mask* is a blank image, *vertices* indicate the x and y coordinates of the edge points to which the polygon is fit, and *ignore_ mask_color* = 255 is the color used to fill the mask.\n",
    "\n",
    "The region mask is applied to the edges using `bitwise_and(img, mask)` function from OpenCV. The output of this step is as shown below:\n",
    "\n",
    "[image6]: ./figures/masked_edges.jpg \"Masked edges\"\n",
    "![alt text][image6]\n",
    "\n",
    "*img* is the image with edges.\n",
    "\n",
    "### 6) Hough line transform\n",
    "\n",
    "Canny edge detector finds edges as points which are strewn together and converted into line segments using Hough transform. The function `HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength, maxLineGap)` from OpenCV runs probabilisitc Hough transform on the canny edges. Probabilistic Hough Transform is an optimization of Hough Transform that does not take all the points into consideration, instead take only a random subset of points and that is sufficient for line detection.The input arguments to the **HoughLinesP()** function are as follows:\n",
    "\n",
    "* rho = 1 (distance resolution in pixels of the Hough grid)\n",
    "* theta = pi/180 (angular resolution in radians of the Hough grid)\n",
    "* threshold = 30 (minimum number of votes (intersections in Hough grid cell))\n",
    "* minLineLength = 10 (minimum number of pixels making up a line; line segments shorter than this are rejected)\n",
    "* maxLineGap = 1 (maximum allowed gap between line segments to treat them as single line)\n",
    "\n",
    "The function outputs _x_ and _y_ coordinates of the start and end of line segments. The OpenCV function `line(img, (x1, y1), (x2, y2), color, thickness)` draws these segments on the image inplace. As can be seen below, the line segments constituting the left and right lanes are broken.\n",
    "\n",
    "[image7]: ./figures/hough_lines.jpg \"Line image\"\n",
    "![alt text][image7]\n",
    "\n",
    "The broken line segments are stitched together to form a consistent lane in the following steps:\n",
    "\n",
    "1. Separate the line segments into left and right line segments based on the sign of their slopes. Only the slope values greater than 'slope_thresh' are retained by getting rid of slope values not corresponding to lanes. This helps removing spurious slope values left out after the region masking step.\n",
    "    \n",
    "2. Calculate the average segment by calculating the average values of each co-ordinate from the segments on left and right side. In other words, calculate the average of x1 values (x1_avg), x2 values (x2_avg), y1 values (y1_avg), and y2 values (y2_avg). Calculate the slope and intercept of the average segment formed by (x1_avg, y1_avg) and (x2_avg, y2_avg).\n",
    "    \n",
    "3. Find the start and end points on each side using the above average slope and intercept values, image size, and end point of region of interest polygon. \n",
    "    \n",
    "4. Draw lines joining the start and end points on each side with given color and thickness. \n",
    "\n",
    "The averaged and extrapolated line segments depicting the lanes can be seen below:\n",
    "\n",
    "[image8]: ./figures/hough_lines_ext.jpg \"Line image with extrapolated lanes\"\n",
    "![alt text][image8]\n",
    "\n",
    "### 7) Overlaying lanes on original image\n",
    "\n",
    "The detected lanes are overlayed on the original image using the function `addWeighted(initial_img, α, img, β, γ)` from OpenCV. The final image is computed as follows: α.initial_img + β.img + γ, and can be seen below:\n",
    "\n",
    "[image9]: ./figures/detected_lanes_ext.jpg \"Original image with superimposed extrapolated lines\"\n",
    "![alt text][image9]\n",
    "\n",
    "*initial_img* is the blank image with averaged and extrapolated line segments depicting the lanes, and *img* is the original image. *α*, *β*, and *γ* are the weights. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <video controls src=\"videos/solidWhiteRight_withDetectedLanes.mp4\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <video controls src=\"videos/solidYellowLeft_withDetectedLanes.mp4\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <video controls src=\"videos/challenge_withDetectedLanes.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of the proposed approach\n",
    "\n",
    "The method assumes a lighting condition such as bright sunlight where the lanes are clearly visible and have very different color and intensity compared to most of the rest of the background. The method also assumes that the lanes are not winding in opposite direction. In other words, the lanes may start as parallel and appear to converge over distance in the image, but they will not cross each other repeatedly in which case the concept of left and right is reversed. If the above assumptions are violated, the method will most likely fail.\n",
    "\n",
    "## Possible improvements\n",
    "\n",
    "The method must be made robust to lighting conditions by applying a transform such as homomorphic filtering which is robust to illumination changes. Transforming the image to be in infrared spectrum might also help as is typically used in military applications to identify criminals in pitch black environment. \n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "I would like to thank Udacity for giving me this opportunity to work on an awesome project. Special thanks to Juan Marcos Ottonello for his article on \"Extrapolating lines\". \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
