{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Mask Detection - Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4bwMEAThppv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Which GPU?\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ylJBFKljXzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Ja-FSokBTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorFlow version\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7D-MqKikkLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMmPKrurkv-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqlPjzzqlNRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lrt \"/content/gdrive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4YnZbW1lSXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install unzip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azuuSQVzlZ63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \"/content/gdrive/My Drive/dataset.zip\" -d /content/dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsCsZ_HEmBK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6CdCEfDnB2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfIW8rZJoPDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All the paths of the flowers\n",
        "ALL_IMG_PATHS = list(paths.list_images(\"dataset\"))\n",
        "print(ALL_IMG_PATHS[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pfZThlToVGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle the image paths and preview\n",
        "random.shuffle(ALL_IMG_PATHS)\n",
        "ALL_IMG_PATHS[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTlFIyHToYkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize images\n",
        "plt.figure(figsize=(15,15))\n",
        "for i in range(25):\n",
        "    image_path = np.random.choice(ALL_IMG_PATHS)\n",
        "    image = plt.imread(image_path)\n",
        "    image = cv2.resize(image, (128, 128))\n",
        "    # you might want to verify the labels before \n",
        "    # you put this to use\n",
        "    label = image_path.split(\"/\")[1]\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(True)\n",
        "    plt.imshow(image)\n",
        "    plt.xlabel(label)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhzmSo96obda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split image paths into test and train splits\n",
        "testImgs = int(len(ALL_IMG_PATHS) * 0.15)\n",
        "trainImgs = len(ALL_IMG_PATHS) - testImgs\n",
        "trainImgPaths = ALL_IMG_PATHS[:trainImgs]\n",
        "testImgPaths = ALL_IMG_PATHS[trainImgs:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp3tdyGQohcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the directory paths\n",
        "train_dir = \"train\"\n",
        "val_dir = \"val\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHnRUcxSokAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def move_images(imgPaths, outputDir):\n",
        "\t# iterate through the image paths\n",
        "\tfor imagePath in imgPaths:\n",
        "\t\t# extract the label from the current image path\n",
        "\t\tlabel = imagePath.split(\"/\")[1]\n",
        "\n",
        "\t\t# check if a directory for the label exists, if not, create it\n",
        "\t\timageDir = os.path.join(outputDir, label)\n",
        "\t\tif not os.path.exists(imageDir):\n",
        "\t\t\tos.makedirs(imageDir)\n",
        "\n",
        "\t\t# copy the current image to the respective folder\n",
        "\t\tshutil.copy2(imagePath, imageDir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqJn4yRYonf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "move_images(trainImgPaths, train_dir)\n",
        "move_images(testImgPaths, val_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOMY10HGoqeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup data generators\n",
        "train_aug = ImageDataGenerator(rescale=1/255.)\n",
        "val_aug = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "train_gen = train_aug.flow_from_directory(\"train\",\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(224, 224),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=True,\n",
        "\tbatch_size=32\n",
        ")\n",
        "val_gen = train_aug.flow_from_directory(\"val\",\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(224, 224),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TtPOvuEow0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the class labels, these will be required later\n",
        "# Reference: https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf1.ipynb\n",
        "print (train_gen.class_indices)\n",
        "labels = '\\n'.join(sorted(train_gen.class_indices.keys()))\n",
        "with open('facemask_labels.txt', 'w') as f:\n",
        "    f.write(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx51SIp2o1Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat facemask_labels.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORd9QL8vo5Ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the MobileNetV2 model but exclude the classification layers\n",
        "EXTRACTOR = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "                    input_shape=(224, 224, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgD5RGPHo9dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_model():\n",
        "    # We are fine-tuning the extractor model\n",
        "    EXTRACTOR.trainable = True\n",
        "    # Construct the head of the model that will be placed on top of the\n",
        "    # the base model\n",
        "    class_head = EXTRACTOR.output\n",
        "    class_head = GlobalAveragePooling2D()(class_head)\n",
        "    class_head = Dense(512, activation=\"relu\")(class_head)\n",
        "    class_head = Dropout(0.5)(class_head)\n",
        "    class_head = Dense(2, activation=\"softmax\")(class_head)\n",
        "\n",
        "    # Create the new model\n",
        "    classifier = Model(inputs=EXTRACTOR.input, outputs=class_head)\n",
        "\n",
        "    # Compile and return the model\n",
        "    classifier.compile(loss=\"categorical_crossentropy\", \n",
        "                          optimizer=\"adam\",\n",
        "                          metrics=[\"accuracy\"])\n",
        "\n",
        "    return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rInLTwFqpCAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LR schedule configuration\n",
        "start_lr = 0.00001\n",
        "min_lr = 0.00001\n",
        "max_lr = 0.00005\n",
        "rampup_epochs = 5\n",
        "sustain_epochs = 0\n",
        "exp_decay = .8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a97WmNiqpEpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LR schedule\n",
        "def lrfn(epoch):\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "        return lr\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "    \n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WH9H-efpIDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How does the LR schedule looks like?\n",
        "rng = [i for i in range(10)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "plt.plot(rng, [lrfn(x) for x in rng])\n",
        "print(y[0], y[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPKJRaZwpKG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(list(paths.list_images(\"train\"))), len(list(paths.list_images(\"val\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6if1yS8rsbHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = len(list(paths.list_images(\"train\")))\n",
        "trainY = len(list(paths.list_images(\"val\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qofbkfbpM2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "facemask_model = get_training_model()\n",
        "start = time.time()\n",
        "history = facemask_model.fit(train_gen,\n",
        "              steps_per_epoch=trainX//32,\n",
        "              validation_data=val_gen,\n",
        "              validation_steps=trainY//32,\n",
        "              epochs=10,\n",
        "              callbacks=[lr_callback])\n",
        "print(\"Total training time: \",time.time()-start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV-HS1MKpTcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting the losses\n",
        "N = len(history.history[\"loss\"])\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KvrdxNjs4zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the image paths from the train set, shuffle them, and \n",
        "# choose 100 images\n",
        "image_paths = list(paths.list_images(\"train\"))\n",
        "random.shuffle(image_paths)\n",
        "image_paths = image_paths[:100]\n",
        "\n",
        "# An empty list as a placeholder for the dataset\n",
        "rep_ds = []\n",
        "\n",
        "# Iterate over the image paths\n",
        "for image in tqdm(image_paths):\n",
        "    # Read the image from the current path, change the datatype, resize the image,\n",
        "    # add batch dimension, normalize the pixel values\n",
        "    image_pixels = plt.imread(image).astype(\"float32\")\n",
        "    image_pixels = cv2.resize(image_pixels, (224, 224))\n",
        "    image_pixels = np.expand_dims(image_pixels, 0)\n",
        "    image_pixels = image_pixels / 255.\n",
        "    \n",
        "    # Append to the list\n",
        "    rep_ds.append(image_pixels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_CroCFzs_Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to TensorFlow dataset\n",
        "rep_ds = np.array(rep_ds)\n",
        "rep_ds = tf.data.Dataset.from_tensor_slices((rep_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrSWkuVNtCGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a representative dataset for int quantization\n",
        "def representative_dataset():\n",
        "    for image in rep_ds.take(100):\n",
        "        yield [image]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z18FNiVDtETe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TF Lite conversion \n",
        "\n",
        "# Instantiate the converter, instruct TF Lite to optimize for size, and\n",
        "# specify the representative dataset\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(facemask_model) \n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] \n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "# We are going for full INT8 quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "# Convert finally\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83cMmQGItGwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unoptimized (SavedModel format)\n",
        "facemask_model.save(\"facemask_model_no_op\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cYLNYIItQCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check size\n",
        "!du --all -h facemask_model_no_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKwQ0ug9tVWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Serialize the TFLite model and check it's size\n",
        "!mkdir tflite_model\n",
        "\n",
        "f = open(\"tflite_model/facemask_model.tflite\", \"wb\")\n",
        "f.write(tflite_model)\n",
        "f.close()\n",
        "\n",
        "!du --all -h tflite_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xJ0NPtvtajh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model into interpreters\n",
        "interpreter_quant = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter_quant.allocate_tensors()\n",
        "input_index_quant = interpreter_quant.get_input_details()[0][\"index\"]\n",
        "output_index_quant = interpreter_quant.get_output_details()[0][\"index\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S-VHwgftoJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare validation sets\n",
        "# Extract the image paths from the train set\n",
        "image_paths = list(paths.list_images(\"val\"))\n",
        "\n",
        "# Empty labels for storing images and labels\n",
        "val_images = []\n",
        "val_labels = []\n",
        "\n",
        "# Iterate over the image paths\n",
        "for image in tqdm(image_paths):\n",
        "    # Read the image from the current path, change the datatype, resize the image,\n",
        "    # add batch dimension, normalize the pixel values\n",
        "    image_pixels = plt.imread(image).astype(\"float32\")\n",
        "    image_pixels = cv2.resize(image_pixels, (224, 224))\n",
        "    image_pixels = np.expand_dims(image_pixels, 0)\n",
        "    image_pixels = image_pixels / 255.\n",
        "\n",
        "    # Extract the label\n",
        "    label = image.split(\"/\")[1]\n",
        "    \n",
        "    # Append to the list\n",
        "    val_images.append(image_pixels)\n",
        "    val_labels.append(label)\n",
        "\n",
        "# Create NumPy array\n",
        "val_images = np.array(val_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeZWsjqEtrWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the class labels (should be alphabetical)\n",
        "CLASSES = ['with_mask', 'without_mask']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4t-fzuYtxqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run a demo prediction\n",
        "test_image = val_images[0]\n",
        "interpreter_quant.set_tensor(input_index_quant, test_image)\n",
        "interpreter_quant.invoke()\n",
        "predictions = interpreter_quant.get_tensor(output_index_quant)\n",
        "\n",
        "# See the prediction\n",
        "plt.imshow(val_images[0].squeeze())\n",
        "template = \"True:{true}, predicted:{predict}\"\n",
        "_ = plt.title(template.format(true=val_labels[0],\n",
        "                              predict=str(CLASSES[np.argmax(predictions[0])])))\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-y0mg83xY_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run a demo prediction\n",
        "test_image = val_images[205]\n",
        "interpreter_quant.set_tensor(input_index_quant, test_image)\n",
        "interpreter_quant.invoke()\n",
        "predictions = interpreter_quant.get_tensor(output_index_quant)\n",
        "\n",
        "# See the prediction\n",
        "plt.imshow(val_images[205].squeeze())\n",
        "template = \"True:{true}, predicted:{predict}\"\n",
        "_ = plt.title(template.format(true=val_labels[0],\n",
        "                              predict=str(CLASSES[np.argmax(predictions[0])])))\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg9HDhNvt2fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(interpreter):\n",
        "  accurate_count = 0\n",
        "  \n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  predictions = []\n",
        "  for val_image, val_label in zip(val_images, val_labels):\n",
        "    interpreter.set_tensor(input_index, val_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    probability = interpreter.get_tensor(output_index)\n",
        "    facemask_id = np.argmax(probability[0])\n",
        "    predictions.append(facemask_id)\n",
        "    \n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    if CLASSES[facemask_id] == val_label:\n",
        "      accurate_count += 1\n",
        "  \n",
        "  accuracy = accurate_count * 1.0 / len(predictions)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmHWUpW_uH-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check, check\n",
        "print(evaluate_model(interpreter_quant))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhianq63uKWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "sudo apt-get update\n",
        "sudo apt-get install edgetpu-compiler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H74h8ItsvZ97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!edgetpu_compiler /content/tflite_model/facemask_model.tflite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxj0pl09vcjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lrt /content/tflite_model/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmoaUR3Fvjp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"tflite_model/facemask_model.tflite\")\n",
        "files.download(\"facemask_model_edgetpu.tflite\")\n",
        "files.download(\"facemask_labels.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWszJe8Hv2jD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP09Ec3ZxBDi",
        "colab_type": "text"
      },
      "source": [
        "We can see that most of the operations of the new compiled model can be executed on an [Edge TPU](https://coral.ai/docs/edgetpu/faq/) - \"Number of operations that will run on Edge TPU: 71\".\n",
        "\n",
        "You can plug an Edge TPU USB accelerator in a supported device (supports Raspberry Pi as well :D) and follow [this documentation](https://coral.ai/docs/reference/edgetpu.classification.engine/) and [this tutorial](https://github.com/google-coral/tflite/blob/master/python/examples/classification/classify_image.py) to use the model we just compiled. Here's the sample inference script that you can use:\n",
        "\n",
        "```python\n",
        "# Usage\n",
        "# python classify_image.py --image 30.jpg\n",
        "# once can change the input image path\n",
        "\n",
        "# Imports\n",
        "import tflite_runtime.interpreter as tflite\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "\n",
        "# Utility function for loading and processing a given image\n",
        "def load_and_preprocess(image_path):\n",
        "\timage_pixels = plt.imread(image_path).astype(\"float32\")\n",
        "\timage_pixels = cv2.resize(image_pixels, (224, 224))\n",
        "\timage_pixels = np.expand_dims(image_pixels, 0)\n",
        "\timage_pixels = image_pixels / 255.\n",
        "\n",
        "\treturn image_pixels\n",
        "\n",
        "# Argument parser\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "\thelp=\"path to input image\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# Load a sample image\n",
        "image_pixels = load_and_preprocess(args[\"image\"])\n",
        "\n",
        "# Load the same image for drawing predictions\n",
        "original =cv2.imread(args[\"image\"])\n",
        "original = imutils.resize(original, width=500)\n",
        "\n",
        "# Open and load the label file\n",
        "labels = open(\"facemask_labels.txt\", \"r\")\n",
        "labels = [label.strip() for label in labels.readlines()]\n",
        "\n",
        "# Load the model into interpreters with TFLite delegate\n",
        "interpreter = tflite.Interpreter(model_path=\"facemask_model_edgetpu.tflite\",\n",
        "\texperimental_delegates=[tflite.load_delegate(\"libedgetpu.so.1\")])\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "# Run inference and parse the predictions\n",
        "interpreter.set_tensor(input_index, image_pixels)\n",
        "start = time.time()\n",
        "interpreter.invoke()\n",
        "print(\"Inference took {:.2f} seconds\".format(time.time() - start))\n",
        "predictions = interpreter.get_tensor(output_index)\n",
        "\n",
        "# Draw the predicted label on the image and show it\n",
        "class_label = str(labels[np.argmax(predictions[0])])\n",
        "confidence = float(predictions[0].max())\n",
        "text = \"Label: {} Conf: {}\".format(class_label, confidence * 100)\n",
        "cv2.putText(original, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "\t\t\t0.5, (0, 255, 0), 2)\n",
        "cv2.imshow(\"Image\", original)\n",
        "cv2.waitKey(0)\n",
        "```"
      ]
    }
  ]
}