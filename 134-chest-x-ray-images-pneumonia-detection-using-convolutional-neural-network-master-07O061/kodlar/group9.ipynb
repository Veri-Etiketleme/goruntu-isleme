{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Import The Required Pakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required packages  \n",
    "import torch\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms #Image processing\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable \n",
    "import os  \n",
    "import glob\n",
    "import pathlib \n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 .Chest X-Ray Images (Pneumonia) Dataset\n",
    "\n",
    "It is a validated dataset from Mendeley Data containing 5863 X-Ray images labeled with two categories: Normal/Pneumonia.The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal)<a href=\"https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\"> Chest X-Ray Images</a> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for device \n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to preprocess the data we create a transform function which is a compisation  of many transforms function provided by pytorch.<br>\n",
    "<b>Transform Function</b><br>\n",
    "<ul><li>Input :an PIL image.</li>\n",
    "<li>Output : Transformed version.</li></ul>\n",
    "\n",
    "<ol>\n",
    "  <p><b>Transforms used:</b></p>\n",
    "  <li><code>transforms.Resize()</code>Resize the image because we need all images to be in the same size.</li>\n",
    "  <li><code>transforms.ToTensor()</code>Convert all the image to PyTorch tensors.</li>\n",
    "  <li><code>transforms.Normalize</code>Normalize the images using the mean and std of the dataset.</li>\n",
    "</ol>\n",
    "We combine these transforms to pipeline with  <code>Transforms.Compose</code>, where it clubs all the transforms provided to it, and run them into sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    #1.Resize the image to have the same size                     \n",
    "    [transforms.Resize((150,150)),\n",
    "    #2.Transform the images from numpy array to Tensor\n",
    "     transforms.ToTensor(),#change the pixel range for each color chanel from 0-255 to 0-1\n",
    "    #3.Normalize the images using the mean and std of the dataset\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Dataloading\n",
    "Since the train and testing data are disributed in different folders per lable, we used <b>ImageFolder</b> class to load the data.<br>\n",
    "<ol>\n",
    "    <p><b>ImageFolder Parameters</b></p>\n",
    "    <li>Root (string) – Root directory path.</li>\n",
    "        <li>Transform Function.</li>\n",
    "\n",
    "</ol><br>\n",
    "After loading the dataset, we have to pass it to the <code>Dataloader</code> class for parallelizing the data loading process with the support of automatic batching.We pass the following constructor :\n",
    "<ul>\n",
    "   <li>Dataset _ train_dataset to train_dataloader,test_dataloader to test_dataloader. </li>\n",
    "<li>Batch size – Refers to the number of samples in each batch.</li>\n",
    "<li>Shuffle – Whether you want the data to be reshuffled or not.</li>\n",
    "<li>collate_fn </li>\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##It causes a problem\n",
    "def collate_fn( batch):\n",
    "        new_batch = []\n",
    "        for idx in range(len(batch)):\n",
    "            sample = batch[idx]\n",
    "            new_batch.append(sample)\n",
    "\n",
    "        # scalar output\n",
    "        sample_batch = np.array(new_batch)\n",
    "        sample_batch = torch.FloatTensor(sample_batch)\n",
    "        sample_batch.squeeze_(2)\n",
    "        print(sample_batch)\n",
    "        return sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the train and test data directory \n",
    "train_data_path = \"chest_xray/train\"\n",
    "test_data_path = \"chest_xray/test\"\n",
    "#DataLoader for training and testing data: we feed the data in the form of the dataloader\n",
    "#Test and train datasets \n",
    "train_dataset = ImageFolder(train_data_path,transform= transform)\n",
    "test_dataset = ImageFolder(test_data_path,transform= transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset\n",
    "                  , batch_size=255, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset\n",
    "                  , batch_size=255, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size :  5216\n",
      "Test dataset size :  624\n"
     ]
    }
   ],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_data_path+'/**/*.jpeg'))\n",
    "test_count=len(glob.glob(test_data_path+'/**/*.jpeg'))\n",
    "print(\"Train dataset size : \",train_count)\n",
    "print(\"Test dataset size : \",test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NORMAL': 0, 'PNEUMONIA': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the classes \n",
    "test_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORMAL', 'PNEUMONIA']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes \n",
    "root = pathlib.Path(train_data_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "#there is two classes 'NORMAL', 'PNEUMONIA'.\n",
    "classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "def is_grey_scale(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    w, h = img.size\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            r, g, b = img.getpixel((i,j))\n",
    "            if r != g != b: \n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(is_grey_scale(test_data_path+\"/NORMAL/IM-0013-0001.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Define a Convolutional Neural Network\n",
    "Our model consist of:<br>\n",
    "<ol>\n",
    "    <li>Three Convolutional layers : create a feature map to predict the class probabilities for each feature by applying a filter that scans the whole image, few pixels at a time.<br>\n",
    "    <ul><li>The first argument to it is the number of input channel: in our case it is a 3 input channel.</li>\n",
    "        <li>The second argument is the number of output channels</li>\n",
    "        <li>The kernel_size argument is the size of the convolutional filter.</li>\n",
    "        <li>Padding argument: we calculate using this formula \"((w-f+2P)/s) +1\",where (w:number of the input, f:filter size,p is the padding and s is the stride </li>\n",
    "        <br></ul>\n",
    "    </li>\n",
    "    <li>Batch normalization  functions: as reported in <a href=\"https://arxiv.org/pdf/1502.03167.pdf \">Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift</a> , \"Merely adding Batch Normalization to a state-of-the-art image classification model yields a substantial speedup in training.\"</li>\n",
    "        \n",
    "    \n",
    "   <li>ReLU functions : The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.We use it here becuse it overcomes the vanishing gradient problem, allowing models to learn faster and perform better, which is caused when using the sigmoid and hyperbolic tangent activation functions.</li>\n",
    "    <li> Max pooling operation,it has two arguments: \n",
    "    <ul><li> Pooling size, which is 2 x 2 and hence the argument is 2. </li>\n",
    "        <li>Stride: is the number of pixels shifts over the input matrix.</li>\n",
    "        </ul></li>\n",
    "    \n",
    "  <li>two fully connected layers are created.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForwardFunction \n",
    "After defining the layers.The next step is to define how the data flows through these layers when performing the forward pass through the network, by defining the <b>forward</b> function,it has <b>x</b>: which is the data that is to be passed through the model (i.e. a batch of data).  This output is then fed into the following layer and so on. Note, after self.layer2, we apply a reshaping function to out, which flattens the data dimensions from 75 x 75 x 64 into 360000 x 1. Next, the dropout is applied followed by the two fully connected layers, with the final output being returned from the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the CNN model\n",
    "\n",
    "class CNN(nn.Module): \n",
    "    def __init__(self,num_classes = 2): #['NORMAL', 'PNEUMONIA']\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "    \n",
    "     \n",
    "        #Input shape= (256,3,150,150) -(batch size, number of chanels,hight,wiedth)\n",
    "        self.conv1=nn.Conv2d\n",
    "        (in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) \n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)                                 \n",
    "        self.relu1=nn.ReLU()                                                    \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2,stride = 2)                        \n",
    "\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (256,12,75,75)\n",
    "        \n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (256,32,75,75)\n",
    "        \n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=75 * 75 * 32,out_features=84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Feed forwad function\n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "            \n",
    "            #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "        output=self.fc1(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define an Optimizer, a Loss Function and the Hyperparameters \n",
    "\n",
    "First, we define the hyperparameters of the training.\n",
    "<ol>\n",
    "    <li><b>The learning rate:</b> reflects how much the model is updated per batch.If it is too small, the training proceeds slowly.If it's too large, the weights will be adjusted too much and miss the true minimum loss, or even become unstable.</li>\n",
    "    <li><b>Batch size:</b> The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.</li>\n",
    "    <li>\n",
    "      <b>Epoch size</b>: The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters \n",
    "\n",
    "batch_size = 255           #feed the batch size according to the cpu or gpu memory\n",
    "lr = 0.0001                # Define a learning rate.\n",
    "num_epochs = 10            # Maximum training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the model, we have to first create an instance of our CNN class, and define our loss function and optimizer.<br>\n",
    "We chose the Cross-entropy loss as a loss function, since we deal with a classification problem. And as optimizer Adam optimizer function. <br>\n",
    "<b>Cross-entropy loss function</b>: measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. <br>\n",
    "<b>Adam optimizer function:</b> we pass the model parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CNN instance\n",
    "model=CNN().to(device)\n",
    "# Loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/21], Loss: 4.7874, Accuracy: 0.00%\n",
      "Epoch [1/10], Step [2/21], Loss: 0.7041, Accuracy: 51.37%\n",
      "Epoch [1/10], Step [3/21], Loss: 1.0373, Accuracy: 76.08%\n",
      "Epoch [1/10], Step [4/21], Loss: 0.8068, Accuracy: 74.51%\n",
      "Epoch [1/10], Step [5/21], Loss: 0.3660, Accuracy: 85.49%\n",
      "Epoch [1/10], Step [6/21], Loss: 0.4615, Accuracy: 80.78%\n",
      "Epoch [1/10], Step [7/21], Loss: 0.2203, Accuracy: 92.55%\n",
      "Epoch [1/10], Step [8/21], Loss: 0.1597, Accuracy: 92.16%\n",
      "Epoch [1/10], Step [9/21], Loss: 0.2793, Accuracy: 89.80%\n",
      "Epoch [1/10], Step [10/21], Loss: 0.2262, Accuracy: 90.59%\n",
      "Epoch [1/10], Step [11/21], Loss: 0.2640, Accuracy: 90.20%\n",
      "Epoch [1/10], Step [12/21], Loss: 0.1688, Accuracy: 92.55%\n",
      "Epoch [1/10], Step [13/21], Loss: 0.1635, Accuracy: 93.33%\n",
      "Epoch [1/10], Step [14/21], Loss: 0.3535, Accuracy: 87.06%\n",
      "Epoch [1/10], Step [15/21], Loss: 0.1740, Accuracy: 92.55%\n",
      "Epoch [1/10], Step [16/21], Loss: 0.2353, Accuracy: 90.20%\n",
      "Epoch [1/10], Step [17/21], Loss: 0.1216, Accuracy: 94.51%\n",
      "Epoch [1/10], Step [18/21], Loss: 0.1649, Accuracy: 93.73%\n",
      "Epoch [1/10], Step [19/21], Loss: 0.1876, Accuracy: 92.94%\n",
      "Epoch [1/10], Step [20/21], Loss: 0.1831, Accuracy: 94.12%\n",
      "Epoch [1/10], Step [21/21], Loss: 0.1207, Accuracy: 94.83%\n",
      "Epoch [2/10], Step [1/21], Loss: 0.0897, Accuracy: 96.86%\n",
      "Epoch [2/10], Step [2/21], Loss: 0.1038, Accuracy: 94.90%\n",
      "Epoch [2/10], Step [3/21], Loss: 0.1335, Accuracy: 94.90%\n",
      "Epoch [2/10], Step [4/21], Loss: 0.1145, Accuracy: 96.08%\n",
      "Epoch [2/10], Step [5/21], Loss: 0.0998, Accuracy: 96.47%\n",
      "Epoch [2/10], Step [6/21], Loss: 0.0949, Accuracy: 95.29%\n",
      "Epoch [2/10], Step [7/21], Loss: 0.0671, Accuracy: 96.86%\n",
      "Epoch [2/10], Step [8/21], Loss: 0.1158, Accuracy: 95.29%\n",
      "Epoch [2/10], Step [9/21], Loss: 0.0520, Accuracy: 98.04%\n",
      "Epoch [2/10], Step [10/21], Loss: 0.1050, Accuracy: 94.90%\n",
      "Epoch [2/10], Step [11/21], Loss: 0.0654, Accuracy: 96.86%\n",
      "Epoch [2/10], Step [12/21], Loss: 0.0369, Accuracy: 99.22%\n",
      "Epoch [2/10], Step [13/21], Loss: 0.0675, Accuracy: 97.65%\n",
      "Epoch [2/10], Step [14/21], Loss: 0.0630, Accuracy: 98.04%\n",
      "Epoch [2/10], Step [15/21], Loss: 0.0608, Accuracy: 96.86%\n",
      "Epoch [2/10], Step [16/21], Loss: 0.1058, Accuracy: 96.08%\n",
      "Epoch [2/10], Step [17/21], Loss: 0.0874, Accuracy: 97.65%\n",
      "Epoch [2/10], Step [18/21], Loss: 0.0593, Accuracy: 97.65%\n",
      "Epoch [2/10], Step [19/21], Loss: 0.1087, Accuracy: 95.69%\n",
      "Epoch [2/10], Step [20/21], Loss: 0.0711, Accuracy: 96.47%\n",
      "Epoch [2/10], Step [21/21], Loss: 0.1047, Accuracy: 96.55%\n",
      "Epoch [3/10], Step [1/21], Loss: 0.0452, Accuracy: 98.82%\n",
      "Epoch [3/10], Step [2/21], Loss: 0.0606, Accuracy: 97.65%\n",
      "Epoch [3/10], Step [3/21], Loss: 0.0868, Accuracy: 96.47%\n",
      "Epoch [3/10], Step [4/21], Loss: 0.0569, Accuracy: 97.65%\n",
      "Epoch [3/10], Step [5/21], Loss: 0.0522, Accuracy: 98.04%\n",
      "Epoch [3/10], Step [6/21], Loss: 0.0402, Accuracy: 98.04%\n",
      "Epoch [3/10], Step [7/21], Loss: 0.0475, Accuracy: 98.04%\n",
      "Epoch [3/10], Step [8/21], Loss: 0.0458, Accuracy: 98.04%\n",
      "Epoch [3/10], Step [9/21], Loss: 0.0467, Accuracy: 97.65%\n",
      "Epoch [3/10], Step [10/21], Loss: 0.0723, Accuracy: 97.25%\n",
      "Epoch [3/10], Step [11/21], Loss: 0.0949, Accuracy: 96.47%\n",
      "Epoch [3/10], Step [12/21], Loss: 0.0254, Accuracy: 99.22%\n",
      "Epoch [3/10], Step [13/21], Loss: 0.0608, Accuracy: 98.04%\n",
      "Epoch [3/10], Step [14/21], Loss: 0.0250, Accuracy: 99.61%\n",
      "Epoch [3/10], Step [15/21], Loss: 0.0649, Accuracy: 97.65%\n",
      "Epoch [3/10], Step [16/21], Loss: 0.0456, Accuracy: 98.43%\n",
      "Epoch [3/10], Step [17/21], Loss: 0.0523, Accuracy: 96.86%\n",
      "Epoch [3/10], Step [18/21], Loss: 0.0419, Accuracy: 99.22%\n",
      "Epoch [3/10], Step [19/21], Loss: 0.0576, Accuracy: 98.43%\n",
      "Epoch [3/10], Step [20/21], Loss: 0.0372, Accuracy: 98.82%\n",
      "Epoch [3/10], Step [21/21], Loss: 0.0316, Accuracy: 98.28%\n",
      "Epoch [4/10], Step [1/21], Loss: 0.0417, Accuracy: 98.43%\n",
      "Epoch [4/10], Step [2/21], Loss: 0.0297, Accuracy: 99.22%\n",
      "Epoch [4/10], Step [3/21], Loss: 0.0506, Accuracy: 97.65%\n",
      "Epoch [4/10], Step [4/21], Loss: 0.0438, Accuracy: 98.82%\n",
      "Epoch [4/10], Step [5/21], Loss: 0.0508, Accuracy: 97.65%\n",
      "Epoch [4/10], Step [6/21], Loss: 0.0272, Accuracy: 99.61%\n",
      "Epoch [4/10], Step [7/21], Loss: 0.0329, Accuracy: 98.82%\n",
      "Epoch [4/10], Step [8/21], Loss: 0.0389, Accuracy: 98.82%\n",
      "Epoch [4/10], Step [9/21], Loss: 0.0204, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [10/21], Loss: 0.0475, Accuracy: 98.04%\n",
      "Epoch [4/10], Step [11/21], Loss: 0.0209, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [12/21], Loss: 0.0333, Accuracy: 99.22%\n",
      "Epoch [4/10], Step [13/21], Loss: 0.0537, Accuracy: 98.43%\n",
      "Epoch [4/10], Step [14/21], Loss: 0.0320, Accuracy: 98.82%\n",
      "Epoch [4/10], Step [15/21], Loss: 0.0457, Accuracy: 98.82%\n",
      "Epoch [4/10], Step [16/21], Loss: 0.0248, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [17/21], Loss: 0.0398, Accuracy: 98.43%\n",
      "Epoch [4/10], Step [18/21], Loss: 0.0310, Accuracy: 99.22%\n",
      "Epoch [4/10], Step [19/21], Loss: 0.0353, Accuracy: 98.82%\n",
      "Epoch [4/10], Step [20/21], Loss: 0.0252, Accuracy: 99.61%\n",
      "Epoch [4/10], Step [21/21], Loss: 0.0653, Accuracy: 98.28%\n",
      "Epoch [5/10], Step [1/21], Loss: 0.0277, Accuracy: 99.61%\n",
      "Epoch [5/10], Step [2/21], Loss: 0.0167, Accuracy: 99.61%\n",
      "Epoch [5/10], Step [3/21], Loss: 0.0226, Accuracy: 99.22%\n",
      "Epoch [5/10], Step [4/21], Loss: 0.0154, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [5/21], Loss: 0.0393, Accuracy: 99.22%\n",
      "Epoch [5/10], Step [6/21], Loss: 0.0280, Accuracy: 99.61%\n",
      "Epoch [5/10], Step [7/21], Loss: 0.0242, Accuracy: 99.22%\n",
      "Epoch [5/10], Step [8/21], Loss: 0.0180, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [9/21], Loss: 0.0179, Accuracy: 99.22%\n",
      "Epoch [5/10], Step [10/21], Loss: 0.0264, Accuracy: 99.61%\n",
      "Epoch [5/10], Step [11/21], Loss: 0.0190, Accuracy: 99.61%\n",
      "Epoch [5/10], Step [12/21], Loss: 0.0213, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [13/21], Loss: 0.0281, Accuracy: 99.61%\n",
      "Epoch [5/10], Step [14/21], Loss: 0.0194, Accuracy: 99.61%\n",
      "Epoch [5/10], Step [15/21], Loss: 0.0282, Accuracy: 99.22%\n",
      "Epoch [5/10], Step [16/21], Loss: 0.0309, Accuracy: 99.22%\n",
      "Epoch [5/10], Step [17/21], Loss: 0.0175, Accuracy: 99.61%\n",
      "Epoch [5/10], Step [18/21], Loss: 0.0448, Accuracy: 98.04%\n",
      "Epoch [5/10], Step [19/21], Loss: 0.0332, Accuracy: 98.82%\n",
      "Epoch [5/10], Step [20/21], Loss: 0.0225, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [21/21], Loss: 0.0798, Accuracy: 96.55%\n",
      "Epoch [6/10], Step [1/21], Loss: 0.0231, Accuracy: 99.22%\n",
      "Epoch [6/10], Step [2/21], Loss: 0.0191, Accuracy: 99.61%\n",
      "Epoch [6/10], Step [3/21], Loss: 0.0241, Accuracy: 99.61%\n",
      "Epoch [6/10], Step [4/21], Loss: 0.0287, Accuracy: 99.61%\n",
      "Epoch [6/10], Step [5/21], Loss: 0.0205, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [6/21], Loss: 0.0139, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [7/21], Loss: 0.0199, Accuracy: 99.61%\n",
      "Epoch [6/10], Step [8/21], Loss: 0.0278, Accuracy: 99.22%\n",
      "Epoch [6/10], Step [9/21], Loss: 0.0224, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [10/21], Loss: 0.0130, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [11/21], Loss: 0.0161, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [12/21], Loss: 0.0224, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [13/21], Loss: 0.0262, Accuracy: 99.61%\n",
      "Epoch [6/10], Step [14/21], Loss: 0.0185, Accuracy: 99.22%\n",
      "Epoch [6/10], Step [15/21], Loss: 0.0200, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [16/21], Loss: 0.0305, Accuracy: 99.22%\n",
      "Epoch [6/10], Step [17/21], Loss: 0.0176, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [18/21], Loss: 0.0164, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [19/21], Loss: 0.0129, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [20/21], Loss: 0.0206, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [21/21], Loss: 0.0204, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [1/21], Loss: 0.0178, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [2/21], Loss: 0.0117, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [3/21], Loss: 0.0155, Accuracy: 99.61%\n",
      "Epoch [7/10], Step [4/21], Loss: 0.0108, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [5/21], Loss: 0.0233, Accuracy: 99.61%\n",
      "Epoch [7/10], Step [6/21], Loss: 0.0160, Accuracy: 99.61%\n",
      "Epoch [7/10], Step [7/21], Loss: 0.0146, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [8/21], Loss: 0.0174, Accuracy: 99.61%\n",
      "Epoch [7/10], Step [9/21], Loss: 0.0145, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [10/21], Loss: 0.0114, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [11/21], Loss: 0.0172, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [12/21], Loss: 0.0156, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [13/21], Loss: 0.0103, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [14/21], Loss: 0.0160, Accuracy: 99.61%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [15/21], Loss: 0.0170, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [16/21], Loss: 0.0203, Accuracy: 99.61%\n",
      "Epoch [7/10], Step [17/21], Loss: 0.0160, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [18/21], Loss: 0.0141, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [19/21], Loss: 0.0152, Accuracy: 99.61%\n",
      "Epoch [7/10], Step [20/21], Loss: 0.0125, Accuracy: 99.61%\n",
      "Epoch [7/10], Step [21/21], Loss: 0.0127, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [1/21], Loss: 0.0122, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [2/21], Loss: 0.0064, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [3/21], Loss: 0.0106, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [4/21], Loss: 0.0108, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [5/21], Loss: 0.0121, Accuracy: 99.61%\n",
      "Epoch [8/10], Step [6/21], Loss: 0.0109, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [7/21], Loss: 0.0099, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [8/21], Loss: 0.0128, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [9/21], Loss: 0.0183, Accuracy: 99.61%\n",
      "Epoch [8/10], Step [10/21], Loss: 0.0143, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [11/21], Loss: 0.0158, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [12/21], Loss: 0.0088, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [13/21], Loss: 0.0125, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [14/21], Loss: 0.0070, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [15/21], Loss: 0.0079, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [16/21], Loss: 0.0134, Accuracy: 99.61%\n",
      "Epoch [8/10], Step [17/21], Loss: 0.0078, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [18/21], Loss: 0.0084, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [19/21], Loss: 0.0176, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [20/21], Loss: 0.0119, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [21/21], Loss: 0.0094, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [1/21], Loss: 0.0091, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [2/21], Loss: 0.0064, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [3/21], Loss: 0.0131, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [4/21], Loss: 0.0133, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [5/21], Loss: 0.0105, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [6/21], Loss: 0.0077, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [7/21], Loss: 0.0075, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [8/21], Loss: 0.0086, Accuracy: 99.61%\n",
      "Epoch [9/10], Step [9/21], Loss: 0.0092, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [10/21], Loss: 0.0081, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [11/21], Loss: 0.0059, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [12/21], Loss: 0.0101, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [13/21], Loss: 0.0093, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [14/21], Loss: 0.0141, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [15/21], Loss: 0.0090, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [16/21], Loss: 0.0104, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [17/21], Loss: 0.0116, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [18/21], Loss: 0.0081, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [19/21], Loss: 0.0073, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [20/21], Loss: 0.0112, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [21/21], Loss: 0.0065, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [1/21], Loss: 0.0063, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [2/21], Loss: 0.0061, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [3/21], Loss: 0.0076, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [4/21], Loss: 0.0158, Accuracy: 99.61%\n",
      "Epoch [10/10], Step [5/21], Loss: 0.0086, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [6/21], Loss: 0.0099, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [7/21], Loss: 0.0081, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [8/21], Loss: 0.0083, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [9/21], Loss: 0.0076, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [10/21], Loss: 0.0067, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [11/21], Loss: 0.0098, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [12/21], Loss: 0.0087, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [13/21], Loss: 0.0083, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [14/21], Loss: 0.0088, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [15/21], Loss: 0.0086, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [16/21], Loss: 0.0085, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [17/21], Loss: 0.0051, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [18/21], Loss: 0.0071, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [19/21], Loss: 0.0078, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [20/21], Loss: 0.0055, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [21/21], Loss: 0.0081, Accuracy: 100.00%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "n_total_step =len(train_dataloader)\n",
    "for epoch in range(num_epochs):    # loop over the dataset multiple times(num_epoch times)\n",
    "\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "   \n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_dataloader): #loop over the train_loader to get the different batches from the dataset\n",
    "       \n",
    "    #push the images,labels to the device to get the gpu support if it is available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        \n",
    "        \n",
    "    #forward pass and create the loss  \n",
    "        \n",
    "        outputs=model(images)         #the model  predict the output \n",
    "        loss=loss_function(outputs,labels)  \n",
    "        \n",
    "    #Backward pass and optimize\n",
    "        optimizer.zero_grad()         #Empty the gradients \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        \n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, n_total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "print(\"Finished Training\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save the model\n",
    "PATH = './best_checkpoint.model'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use three evaluation metrices :\n",
    "\n",
    "   <ol>\n",
    "    <li>Accuracy:</li> Accuracy is the quintessential classification metric. It is pretty easy to understand. And easily suited for binary as well as a multiclass classification problem.\n",
    "        <li>Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.</li><ul>\n",
    "    <li>Precision (P) is defined as the number of true positives (TP) over the number of true positives(TP) plus the number of false positives (FP): $$\\frac{TP}{TP+FP}$$. </li>\n",
    "    <li>Recall (R) is defined as the number of true positives (TP) over the number of true positives(TP) plus the number of false negatives (FN): $$\\frac{TP}{TP+FN}$$.</li>\n",
    "    </ul>\n",
    "          \n",
    "   </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 624 test images: 74.19871794871794 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = [] #use it to calculate the Precision and Recall\n",
    "    y_predicated =[] #use it to calculate the Precision and Recall\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_dataloader:\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "       \n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        y_true.append(labels)\n",
    "        y_predicated.append(predicted)\n",
    "        \n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    \n",
    "    print('Accuracy of the network on the 624 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "#torch.save(model.state_dict(), 'best_checkpoint.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##flatten the array of tensors in order to calculate preccision and recall scores\n",
    "y_true_flatten =[]\n",
    "y_predicated_flatten =[]\n",
    "for i in y_true:\n",
    "    for j in i:\n",
    "        y_true_flatten.append(j)\n",
    "for i in y_predicated:\n",
    "    for j in i:\n",
    "        y_predicated_flatten.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.709324\n"
     ]
    }
   ],
   "source": [
    "##### precision tp / (tp + fp)\n",
    "precision = precision_score(y_true_flatten, y_predicated_flatten)\n",
    "print('Precision: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.994872\n"
     ]
    }
   ],
   "source": [
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_true_flatten, y_predicated_flatten)\n",
    "print('Recall: %f' % recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
